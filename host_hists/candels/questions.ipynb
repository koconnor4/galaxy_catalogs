{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Color-color, color-mag classification. What/where is software, should I try and use this at least on a few so I'm aware of how it works? The contours of different types are coming from some sort of combination of the pdf of the colors; what exactly is function that will give contour from these? One further back where do the pdfs come from? Not a model we fit by varying parameters, doesn't make sense for single point right? Is it just raw data from many known SN types by other classifications (probably well studied specs) and colors over different times since peaks binned by redshifts?\n",
    "\n",
    "## 2. Stardust, runs through all models of each type (Ia, Ib/c, II) finding best of each; gives a probability for each type. Where does the probability come from is it the chisq? Why can we compare the chisq for different models in general if some models have more free parameters to vary; is that what the nu is in chisq/nu? I'm assuming probability more complicated than a linear type fraction like  (chisq model/total chisq)? Again for cases where my host has significantly different redshift than evernote/published should I try and use this to see if get better fit?\n",
    "\n",
    "## 3. The classification stats pdf? What am I looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
